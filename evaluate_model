{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 19:51:02.712497: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Map:   0%|          | 0/4094 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 4094/4094 [05:59<00:00, 11.39 examples/s]\n",
      "Using the latest cached version of the module from /home/m_fetrat/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--wer/85bee9e4216a78bb09b2d0d500f6af5c23da58f9210e661add540f5df6630fcd (last modified on Sat Jan 25 12:01:38 2025) since it couldn't be found locally at evaluate-metric--wer, or remotely on the Hugging Face Hub.\n",
      "Evaluating: 100%|██████████| 256/256 [3:08:24<00:00, 44.16s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER on latest checkpoint (/media/external_16TB_1/m_fetrat/speech-project/finetuned/checkpoint-26-1700): 0.2692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "from datasets import load_from_disk\n",
    "from datasets import Audio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load the test dataset\n",
    "test = load_from_disk('/media/external_16TB_1/m_fetrat/speech-project/youtube_data/test')\n",
    "test = test.remove_columns([\"youtube_url\", \"title\", \"segment_id\", \"video_id\"])\n",
    "\n",
    "# Preprocessing function that combines both character removal and feature preparation\n",
    "chars_to_keep = \"حابخدذرزسشصضطظعغفقلئآمتثجنهوپچژکگی‌\"\n",
    "chars_to_ignore_regex = f'[^{chars_to_keep}\\s]'\n",
    "\n",
    "# Load the tokenizer and feature extractor\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"<unk>\", pad_token=\"<pad>\", word_delimiter_token=\"|\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "def preprocess(batch):\n",
    "    # Remove special characters from transcription\n",
    "    batch[\"transcription\"] = re.sub(chars_to_ignore_regex, '', batch[\"transcription\"]).lower() + \" \"\n",
    "\n",
    "    # Process audio to input values\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    \n",
    "    # Tokenize transcription to labels\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"transcription\"]).input_ids\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Apply the combined preprocessing function in one map call\n",
    "test = test.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "test = test.map(preprocess, remove_columns=test.column_names, num_proc=1)\n",
    "\n",
    "# Load the model from the latest checkpoint\n",
    "checkpoint_dir = \"/media/external_16TB_1/m_fetrat/speech-project/finetuned\"\n",
    "latest_checkpoint = \"/media/external_16TB_1/m_fetrat/speech-project/finetuned/checkpoint-26-1700\"\n",
    "model = Wav2Vec2ForCTC.from_pretrained(os.path.join(checkpoint_dir, latest_checkpoint))\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cuda:0\"\n",
    "model.to(device)\n",
    "\n",
    "# DataCollator for padding\n",
    "from typing import List, Dict, Union, Optional\n",
    "\n",
    "class DataCollatorCTCWithPadding:\n",
    "    def __init__(\n",
    "        self,\n",
    "        processor: Wav2Vec2Processor,\n",
    "        padding: Union[bool, str] = True,\n",
    "        max_length: Optional[int] = None,\n",
    "        max_length_labels: Optional[int] = None,\n",
    "        pad_to_multiple_of: Optional[int] = None,\n",
    "        pad_to_multiple_of_labels: Optional[int] = None,\n",
    "    ):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "        self.max_length_labels = max_length_labels\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "        self.pad_to_multiple_of_labels = pad_to_multiple_of_labels\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "# Create DataLoader for evaluation\n",
    "eval_dataloader = DataLoader(test, batch_size=16, collate_fn=data_collator)\n",
    "\n",
    "# Load evaluation metric (WER)\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "wer_scores = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        input_values = batch[\"input_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_values, labels=labels)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Decode predictions and labels\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        pred_str = processor.batch_decode(pred_ids.cpu().numpy())\n",
    "        label_str = processor.batch_decode(labels.cpu().numpy(), group_tokens=False)\n",
    "\n",
    "        # Compute WER\n",
    "        wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "        wer_scores.append(wer)\n",
    "\n",
    "avg_wer = np.mean(wer_scores)\n",
    "print(f\"Test WER on latest checkpoint ({latest_checkpoint}): {avg_wer:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asrvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
